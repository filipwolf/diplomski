@article{alphafold,
  doi = {10.1038/s41586-021-03819-2},
  url = {https://doi.org/10.1038/s41586-021-03819-2},
  year = {2021},
  author = {Jumper, J., Evans, R., Pritzel, A. et al.},
  title = {Highly accurate protein structure prediction with AlphaFold},
  journal = {Nature}
}

@ARTICLE{CNN,
	author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	journal={Proceedings of the IEEE}, 
	title={Gradient-based learning applied to document recognition}, 
	year={1998},
	volume={86},
	number={11},
	pages={2278-2324},
	doi={10.1109/5.726791}
}

@article{uni_approx,
	title = {Multilayer feedforward networks are universal approximators},
	journal = {Neural Networks},
	volume = {2},
	number = {5},
	pages = {359-366},
	year = {1989},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@InProceedings{GNN,
	author="Scarselli, Franco
	and Tsoi, Ah Chung
	and Gori, Marco
	and Hagenbuchner, Markus",
	editor="Fred, Ana
	and Caelli, Terry M.
	and Duin, Robert P. W.
	and Campilho, Aur{\'e}lio C.
	and de Ridder, Dick",
	title="Graphical-Based Learning Environments for Pattern Recognition",
	booktitle="Structural, Syntactic, and Statistical Pattern Recognition",
	year="2004",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="42--56",
	abstract="In this paper, we present a new neural network model, called graph neural network model, which is a generalization of two existing approaches, viz., the graph focused approach, and the node focused approach. The graph focused approach considers the mapping from a graph structure to a real vector, in which the mapping is independent of the particular node involved; while the node focused approach considers the mapping from a graph structure to a real vector, in which the mapping depends on the properties of the node involved. It is shown that the graph neural network model maintains some of the characteristics of the graph focused models and the node focused models respectively. A supervised learning algorithm is derived to estimate the parameters of the graph neural network model. Some experimental results are shown to validate the proposed learning algorithm, and demonstrate the generalization capability of the proposed model.",
	isbn="978-3-540-27868-9"
}


@InProceedings{message_passing,
	title = 	 {Neural Message Passing for Quantum Chemistry},
	author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {1263--1272},
	year = 	 {2017},
	editor = 	 {Precup, Doina and Teh, Yee Whye},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {06--11 Aug},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
	url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
	abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}
