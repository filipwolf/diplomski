@article{alphafold,
  doi = {10.1038/s41586-021-03819-2},
  url = {https://doi.org/10.1038/s41586-021-03819-2},
  year = {2021},
  author = {Jumper, J., Evans, R., Pritzel, A. et al.},
  title = {Highly accurate protein structure prediction with AlphaFold},
  journal = {Nature}
}

@article{CNN,
	author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	journal={Proceedings of the IEEE}, 
	title={Gradient-based learning applied to document recognition}, 
	year={1998},
	volume={86},
	number={11},
	pages={2278-2324},
	doi={10.1109/5.726791}
}

@article{uni_approx,
	title = {Multilayer feedforward networks are universal approximators},
	journal = {Neural Networks},
	volume = {2},
	number = {5},
	pages = {359-366},
	year = {1989},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@InProceedings{GNN,
	author="Scarselli, Franco
	and Tsoi, Ah Chung
	and Gori, Marco
	and Hagenbuchner, Markus",
	editor="Fred, Ana
	and Caelli, Terry M.
	and Duin, Robert P. W.
	and Campilho, Aur{\'e}lio C.
	and de Ridder, Dick",
	title="Graphical-Based Learning Environments for Pattern Recognition",
	booktitle="Structural, Syntactic, and Statistical Pattern Recognition",
	year="2004",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="42--56",
	abstract="In this paper, we present a new neural network model, called graph neural network model, which is a generalization of two existing approaches, viz., the graph focused approach, and the node focused approach. The graph focused approach considers the mapping from a graph structure to a real vector, in which the mapping is independent of the particular node involved; while the node focused approach considers the mapping from a graph structure to a real vector, in which the mapping depends on the properties of the node involved. It is shown that the graph neural network model maintains some of the characteristics of the graph focused models and the node focused models respectively. A supervised learning algorithm is derived to estimate the parameters of the graph neural network model. Some experimental results are shown to validate the proposed learning algorithm, and demonstrate the generalization capability of the proposed model.",
	isbn="978-3-540-27868-9"
}


@InProceedings{message_passing,
	title = 	 {Neural Message Passing for Quantum Chemistry},
	author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {1263--1272},
	year = 	 {2017},
	editor = 	 {Precup, Doina and Teh, Yee Whye},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {06--11 Aug},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
	url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
	abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}

@article{HGP2,
	author = {Sergey Nurk  and Sergey Koren  and Arang Rhie  and Mikko Rautiainen  and Andrey V. Bzikadze  and Alla Mikheenko  and Mitchell R. Vollger  and Nicolas Altemose  and Lev Uralsky  and Ariel Gershman  and Sergey Aganezov  and Savannah J. Hoyt  and Mark Diekhans  and Glennis A. Logsdon  and Michael Alonge  and Stylianos E. Antonarakis  and Matthew Borchers  and Gerard G. Bouffard  and Shelise Y. Brooks  and Gina V. Caldas  and Nae-Chyun Chen  and Haoyu Cheng  and Chen-Shan Chin  and William Chow  and Leonardo G. de Lima  and Philip C. Dishuck  and Richard Durbin  and Tatiana Dvorkina  and Ian T. Fiddes  and Giulio Formenti  and Robert S. Fulton  and Arkarachai Fungtammasan  and Erik Garrison  and Patrick G. S. Grady  and Tina A. Graves-Lindsay  and Ira M. Hall  and Nancy F. Hansen  and Gabrielle A. Hartley  and Marina Haukness  and Kerstin Howe  and Michael W. Hunkapiller  and Chirag Jain  and Miten Jain  and Erich D. Jarvis  and Peter Kerpedjiev  and Melanie Kirsche  and Mikhail Kolmogorov  and Jonas Korlach  and Milinn Kremitzki  and Heng Li  and Valerie V. Maduro  and Tobias Marschall  and Ann M. McCartney  and Jennifer McDaniel  and Danny E. Miller  and James C. Mullikin  and Eugene W. Myers  and Nathan D. Olson  and Benedict Paten  and Paul Peluso  and Pavel A. Pevzner  and David Porubsky  and Tamara Potapova  and Evgeny I. Rogaev  and Jeffrey A. Rosenfeld  and Steven L. Salzberg  and Valerie A. Schneider  and Fritz J. Sedlazeck  and Kishwar Shafin  and Colin J. Shew  and Alaina Shumate  and Ying Sims  and Arian F. A. Smit  and Daniela C. Soto  and Ivan Sović  and Jessica M. Storer  and Aaron Streets  and Beth A. Sullivan  and Françoise Thibaud-Nissen  and James Torrance  and Justin Wagner  and Brian P. Walenz  and Aaron Wenger  and Jonathan M. D. Wood  and Chunlin Xiao  and Stephanie M. Yan  and Alice C. Young  and Samantha Zarate  and Urvashi Surti  and Rajiv C. McCoy  and Megan Y. Dennis  and Ivan A. Alexandrov  and Jennifer L. Gerton  and Rachel J. O’Neill  and Winston Timp  and Justin M. Zook  and Michael C. Schatz  and Evan E. Eichler  and Karen H. Miga  and Adam M. Phillippy },
	title = {The complete sequence of a human genome},
	journal = {Science},
	volume = {376},
	number = {6588},
	pages = {44-53},
	year = {2022},
	doi = {10.1126/science.abj6987},
	URL = {https://www.science.org/doi/abs/10.1126/science.abj6987},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.abj6987},
	abstract = {Since its initial release in 2000, the human reference genome has covered only the euchromatic fraction of the genome, leaving important heterochromatic regions unfinished. Addressing the remaining 8\% of the genome, the Telomere-to-Telomere (T2T) Consortium presents a complete 3.055 billion–base pair sequence of a human genome, T2T-CHM13, that includes gapless assemblies for all chromosomes except Y, corrects errors in the prior references, and introduces nearly 200 million base pairs of sequence containing 1956 gene predictions, 99 of which are predicted to be protein coding. The completed regions include all centromeric satellite arrays, recent segmental duplications, and the short arms of all five acrocentric chromosomes, unlocking these complex regions of the genome to variational and functional studies.}
}

@article{HGP1,
	title={Initial sequencing and analysis of the human genome},
	volume={409},
	DOI={10.1038/35057062},
	number={6822},
	journal={Nature},
	author={Lander, Eric S. and Linton, Lauren M. and Birren, Bruce and Nusbaum, Chad and Zody, Michael C. and Baldwin, Jennifer and Devon, Keri and Dewar, Ken and Doyle, Michael and FitzHugh, William et al.},
	year={2001},
	pages={860-921}
}

@misc{genome_cost,
	author = {NIH},
	title = {The Cost of Sequencing a Human Genome},
	year = {2021},
	url = {https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost}
}

@article{Vaser,
	author = {Vaser, Robert and {\v S}iki{\'c}, Mile},
	title = {Raven: a de novo genome assembler for long reads},
	elocation-id = {2020.08.07.242461},
	year = {2021},
	doi = {10.1101/2020.08.07.242461},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {We present new methods for the improvement of de novo genome assembly from erroneous long-reads incorporated into a straightforward tool called Raven (https://github.com/lbcb-sci/raven). Raven maintains similar performance for various genomes and has accuracy on par with other assemblers which support third-generation sequencing data. It is one of the fastest options while having the lowest memory consumption on the majority of benchmarked datasets.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2021/02/22/2020.08.07.242461},
	eprint = {https://www.biorxiv.org/content/early/2021/02/22/2020.08.07.242461.full.pdf},
	journal = {bioRxiv}
}

@proceedings{DGL,
	doi = {10.48550/ARXIV.1909.01315},
	url = {https://arxiv.org/abs/1909.01315},
	author = {Wang, Minjie and Zheng, Da and Ye, Zihao and Gan, Quan and Li, Mufei and Song, Xiang and Zhou, Jinjing and Ma, Chao and Yu, Lingfan and Gai, Yu and Xiao, Tianjun and He, Tong and Karypis, George and Li, Jinyang and Zhang, Zheng},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},
	publisher = {arXiv},
	year = {2019},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@proceedings{GCN,
	doi = {10.48550/ARXIV.1609.02907},
	url = {https://arxiv.org/abs/1609.02907},
	author = {Kipf, Thomas N. and Welling, Max},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	publisher = {arXiv},
	year = {2016},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@proceedings{GATv2,
	doi = {10.48550/ARXIV.2105.14491},
	url = {https://arxiv.org/abs/2105.14491},
	author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {How Attentive are Graph Attention Networks?},
	publisher = {arXiv},
	year = {2021},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@proceedings{GAT,
	doi = {10.48550/ARXIV.1710.10903},
	url = {https://arxiv.org/abs/1710.10903},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Graph Attention Networks},
	publisher = {arXiv},
	year = {2017},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{de_novo1,
	author = {Melbourne Bioinformatics},
	title = {De novo Genome Assembly for Illumina Data},
	url = {https://www.melbournebioinformatics.org.au/tutorials/tutorials/assembly/assembly-protocol/}
}

@misc{de_novo2,
	author = {Illumina},
	title = {Assembling novel genomes},
	url = {https://www.illumina.com/techniques/sequencing/dna-sequencing/whole-genome-sequencing/de-novo-sequencing.html}
}


@misc{yeast,
	author = {NCBI},
	title = {Saccharomyces cerevisiae},
	url = {https://www.ncbi.nlm.nih.gov/genome/?term=Saccharomyces%20cerevisiae[Organism]&cmd=DetailsSearch#:~:text=The%20Saccharomyces%20cerevisiae%20genome%20is,Mb%2C%20organized%20in%2016%20chromosomes.}
}

@misc{human,
	author = {NCBI},
	title = {Homo sapiens},
	url = {https://www.ncbi.nlm.nih.gov/genome/?term=Homo+sapiens}
}

@misc{haplotype,
	author = {Scitable},
	title = {haplotype},
	url = {https://www.nature.com/scitable/definition/haplotype-haplotypes-142/}
}

@misc{haplotype_usage,
	author = {PacBio},
	title = {Sequencing 101: ploidy, haplotypes, and phasing — how to get more from your sequencing data},
	url = {https://www.pacb.com/blog/ploidy-haplotypes-and-phasing/}
}

@misc{popular_ml,
	author = {Statistics & Data},
	title = {Most Popular Machine Learning Libraries – 2014/2021},
	url = {https://statisticsanddata.org/data/most-popular-machine-learning-libraries/}
}

@misc{dl_applications,
	author = {Marina Chatterjee},
	title = {Top 20 Applications of Deep Learning in 2022 Across Industries},
	year = {2022},
	url = {https://www.mygreatlearning.com/blog/deep-learning-applications/}
}

@misc{CEL,
	author = {Kiprono Elijah Koech},
	title = {Cross-Entropy Loss Function},
	year = {2020},
	url = {https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e}
}

@misc{dl_growth,
	author = {Zion Market Research},
	title = {Outlook on the Global Deep Learning Market Size, Share \& Growth 2022 - 2028 | Estimated to Achieve a Revenue of \$80769.6 Million With Growing at a CAGR 38.3% Report by Zion Market Research},
	year = {2022},
	url = {https://www.prnewswire.com/news-releases/outlook-on-the-global-deep-learning-market-size-share--growth-2022---2028--estimated-to-achieve-a-revenue-of-80769-6-million-with-growing-at-a-cagr-38-3-report-by-zion-market-research-301550212.html#:~:text=As%20per%20the%20analysis%20shared,%25%20(2022%2D2028).}
}

@book{Goodfellow-et-al-2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@article{dl_bioinformatics,
	author = {Min, Seonwoo and Lee, Byunghan and Yoon, Sungroh},
	title = "{Deep learning in bioinformatics}",
	journal = {Briefings in Bioinformatics},
	volume = {18},
	number = {5},
	pages = {851-869},
	year = {2016},
	month = {07},
	abstract = "{In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.}",
	issn = {1467-5463},
	doi = {10.1093/bib/bbw068},
	url = {https://doi.org/10.1093/bib/bbw068},
	eprint = {https://academic.oup.com/bib/article-pdf/18/5/851/25581102/bbw068.pdf},
}

@book{trudeau_2017,
	place={La Vergne},
	title={Introduction to graph theory},
	publisher={Stanford},
	author={Trudeau, Richard J.},
	year={2017}
}

@BOOK{GRL,
	author={Hamilton, William L.},
	booktitle={Graph Representation Learning},
	year={2020},
	doi={10.2200/S01045ED1V01Y202009AIM046}
} 

@proceedings{bronstein2021geometric,
	title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
	author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
	year={2021},
	eprint={2104.13478},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@proceedings{attention,
	doi = {10.48550/ARXIV.1409.0473},
	url = {https://arxiv.org/abs/1409.0473},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Neural Machine Translation by Jointly Learning to Align and Translate},
	publisher = {arXiv},
	year = {2014},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{EGAT,
	author = {Kamiński K, Ludwiczak J, Jasiński M, Bukala A, Madaj R, Szczepaniak K, Dunin-Horkawicz S},
	title = {Rossmann-toolbox: a deep learning-based protocol for the prediction and design of cofactor specificity in Rossmann fold proteins},
	journal = {Brief Bioinform.},
	year = {2022},
	doi = {10.1093/bib/bbab371},
	PMID = {34571541},
	PMCID = {PMC8769691}
}

@proceedings{dropout,
	doi = {10.48550/ARXIV.1207.0580},
	url = {https://arxiv.org/abs/1207.0580},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	keywords = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	publisher = {arXiv},
	year = {2012},
	copyright = {arXiv.org perpetual, non-exclusive license}
}