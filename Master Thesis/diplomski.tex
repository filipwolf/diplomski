\documentclass[times, utf8, diplomski, english]{fer_eng}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage{color}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{tikz} 
\usepackage{pgfplots}
\usepackage{bm}
\usepackage{relsize}
\usepackage{subfigure}
\setcitestyle{numbers}
\setcitestyle{square}
\lstset{
basicstyle=\ttfamily,
breaklines=true,
backgroundcolor=\color{lightgray}
}
\usepackage{listofitems} % for \readlist to create arrays
\usetikzlibrary{arrows.meta} % for arrow size
\usetikzlibrary{positioning}

\usepackage[outline]{contour} % glow around text
\contourlength{1.4pt}

\tikzset{>=latex} % for LaTeX arrow head
\usepackage{xcolor}
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}
\tikzstyle{node}=[thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]
\tikzstyle{node in}=[node,green!20!black,draw=mygreen!30!black,fill=mygreen!25]
\tikzstyle{node hidden}=[node,blue!20!black,draw=myblue!30!black,fill=myblue!20]
\tikzstyle{node convol}=[node,orange!20!black,draw=myorange!30!black,fill=myorange!20]
\tikzstyle{node out}=[node,red!20!black,draw=myred!30!black,fill=myred!20]
\tikzstyle{connect}=[thick,mydarkblue] %,line cap=round
\tikzstyle{connect arrow}=[-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1]
\tikzset{ % node styles, numbered for easy mapping with \nstyle
	node 1/.style={node in},
	node 2/.style={node hidden},
	node 3/.style={node out},
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3
\def\layersep{1.5}
\def\nodesep{1}

\begin{document}

\thesisnumber{3028}

\title{Using Graph Neural Networks to Separate Haplotypes in Assembly Graphs}

\author{Filip Wolf}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\includepdf[pages=-]{"hr_0036510053_56.pdf"}

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{}

\tableofcontents

\chapter{Introduction}

Traditionally, the focus of \textit{de novo} genome assembly has always been on the reconstruction of an individual's genome from its numerous broken-up fragments obtained after sequencing \cite{de_novo2}. We will here, however, focus on a different application of \textit{de novo} genome assembly: haplotype separation. Every individual's genome is composed of both a mother's and a father's genome. Because we inherit both parent's chromosomes, the genetic material between them gets mixed up, but some regions stay together regardless in the form of genes. The term \textit{haplotype} refers both to these inherited regions, as well as to all of the genes of a single parent on a chromosome \cite{haplotype}. In this thesis, we will use the term haplotype in the latter sense. By separating the two haplotypes in a genome, we can determine which parent is responsible for what genes. This has a wide range of applications, from ancestry tests to finding hereditary diseases \cite{haplotype_usage}.         

We will try to do this using novel algorithms from the field of \textit{deep learning} (DL), which is itself a subfield of \textit{machine learning} (ML). Bioinformatics has long been dominated by algorithms that employ complex heuristics and expert knowledge to find solutions to the problems researchers face \cite{compeau_pevzner_2015}. This is however slowly changing. More and more research is being done using ML to solve problems in bioinformatics, foregoing the laborious process of feature engineering and extensive human intervention. First, ML was employed only for finding dense and abstract representations of genome features, but it later started to completely replace the previously mentioned algorithms. It has since contributed tremendously to the field and shows no signs of stopping, the most notable achievement being the solution to the protein folding problem which previously wasn't solvable for 50 years \cite{alphafold}. Still, there remains a long way to go before ML, and to an extent, DL, becomes completely standard within the field. Thus, this thesis is concerned with applying recent DL techniques in order to solve the problem of separating haplotypes in genomes. This could not only potentially improve existing solutions to the problem at hand and reduce the necessity for human expert intervention, but also bring bioinformatics to a wider range of people who may not necessarily have a deep understanding of genetics \cite{dl_bioinformatics}.

\section{Bioinformatics}

Bioinformatics is an interdisciplinary field of research that has had a tremendous impact on humanity in the last few decades. Since the completion of the Human Genome Project \cite{HGP1} \cite{HGP2}, the cost of sequencing a human genome has fallen exponentially. We can now reliably sequence a human genome for less than a \$1000 \cite{genome_cost}, all thanks to recent advances in sequencing technology, as well as accompanying algorithms. In section \ref{subsec:the process of sequencing and assembly}, we will briefly explain the general pipeline of genome sequencing and assembly, while in section \ref{subsec:file formats}, we will go through all the file formats used in this thesis for storing data	.

\subsection{The Process of Sequencing and Assembly}
\label{subsec:the process of sequencing and assembly}

In an ideal world, we would extract a human genome in the form of DNA from a cell, input it into a \textit{sequencer}, and get a complete and accurate sequence of a DNA molecule as output with no \textit{base pairs} (adenine, cytosine, guanine and thymine) missing, which we could immediately use for further studying. Unfortunately, this perfect process is not (yet) a reality and it is hard to predict when this might become so. Due to this, we have to make due with sequencers that can only output genomes in the form of thousands of fragmented \textit{reads}, at maximum about 10 kilobases (kb) long, with shorter read sequencers sequencing reads at lengths of around 150 base pairs (bp). This process is called \textit{shotgun sequencing}. An average genome is much longer than that, e.g. the yeast genome is around 12 Mb long \cite{yeast} and the human genome is around 6.4 Gb long \cite{human}, so after sequencing, we need to assemble these short reads into longer ones before going further into analysis.

This process of finding longer reads is the first step in the process of \textit{assembly}, depicted as steps one through three in Figure \ref{fig:genome assembly}. These longer reads are called \textit{contigs}. We can use the overlaps between sequences (step two in Figure \ref{fig:genome assembly}) to build graphs in which each read is represented with a node, and each overlap between reads is represented with an edge in the graph. An overlap can be described as a match between two contigs' \textit{ends}, i.e. the contigs' prefix (start) and suffix (end), which can come in the form of a prefix - suffix or an suffix - prefix overlap. The length of this overlap can indicate a stronger similarity between two reads, and therefore a stronger link in the graph between them. Finally, we are tasked with finding the longest possible path on this graph to connect the individual contigs and form a complete genome. We also need to orient them properly, depicted as step four in Figure \ref{fig:genome assembly}.

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/genome_assembly.png}
	\caption[Genome assembly]{An illustration depicting the process of \textit{de novo} genome assembly in four steps. First, fragmented sequences are obtained after sequencing. Next, we find overlaps between the sequences. We then combine these overlaps into contigs, and finally, determine the contigs' orientation to create scaffolds. This illustration was inspired by \cite{baker_2012}.}
	\label{fig:genome assembly}
\end{figure}

\subsection{File Formats}
\label{subsec:file formats}

As a distinct subfield that combines computer science and genetics, bioinformatics uses multiple file formats that are unique to it. Mostly used for storing representations of genomic data, they offer us a simpler way to work genome sequences, as well as modify them. In the following sections, we will briefly describe these file formats, as well as state their usage in this thesis.

\subsubsection{FASTA \& FASTQ}
\label{subsubsec:fasta and fastq}

The most common file format encountered while working with genomic data is the FASTA format, designated by its .fasta or .fa file extension. It is a text-based format for storing nucleotide sequences and their information. Base pairs of amino acids are stored using letters (A - adenine, C - cytosine, G - guanine, T - thymine) in the form of a long sequence. Every sequence starts with a description line, which is designated with a ">" symbol at the start of the line. The description line can contain various information about the sequence, such as sequence name, sequence length, sequence statistics, etc. In our case, it specified the species and chromosome to which the sequences belonged to (section \ref{sec:introduction}). After generating the reads for our data (Chapter \ref{ch:dataset}), the description contained data such as the read number, strand information (forward or reverse), position on the chromosome, length and the chromosome to which it belonged to, as well as if it was a mutated read or not. Sometimes, we may also encounter a FASTQ file, which is identical to a FASTA file, but with added sequence quality score information added after every sequence. In the following example, we can see a typical line of a FASTA file filled with short reads:

\begin{lstlisting}
>read=1,reverse,position=1004750-1015277,length=10527,NC_001139.9
AAGCTTGCAGATTTATTAACAGTTCAAACGAGTTTGGCTGATAATGCTCGTGCAGGTATTG
\end{lstlisting}

\subsubsection{CSV}
\label{subsubsec:csv}

After generating various information about our dataset, we would store it in a CSV (comma separated values) file. That way, it could be  more easily read by some of the tools we used for extracting data from reads (section \ref{sec:technology stack}). A CSV file can be thought of as a table, where values in a row are separated with a comma sign. The file starts with a header with column names that are also separated with commas.

Another reason why we use the CSV file format is because of the Raven assembler (section \ref{subsec:assembling the graph}) outputting a CSV file with information about sequence overlap lengths. This information is stored in the file in the following way. We start in a descending order of read name. A line with an even index number represents an original read, while a line with an odd index number represents a \textit{virtual} read, i.e. a \textit{reverse complement} or \textit{strand} of the original read. A reverse complement represents the same sequence as the original, but in reverse order and with the base pairs replaced with their complements (A - T, C - G). The original and the reverse complement essentially represent the same genomic data, but on different strands of the DNA, which are either a forward or backward strand. An example of such a file ican be seein in the following example:

\begin{lstlisting}
6549 [3274] LN:i:9216 RC:i:1,6412 [3206] LN:i:11360 RC:i:1,1,39678 8937 0 0.992832
6413 [3206] LN:i:11360 RC:i:1,6548 [3274] LN:i:9216 RC:i:1,1,39679 11081 0 0.992832
6647 [3323] LN:i:10656 RC:i:1,6638 [3319] LN:i:10224 RC:i:1,1,39680 10347 0 0.990291
6639 [3319] LN:i:10224 RC:i:1,6646 [3323] LN:i:10656 RC:i:1,1,39681 9915 0 0.990291
\end{lstlisting}

First, we have the index of the first read. Then, in square brackets, we have the index of the original first read (each virtual read forms a pair with an original read). We then have some read information, followed by the second read with which the first one forms an overlap, along with its own information. Lastly, we have three fields that represent the overlap length, the weight of the overlap (not used in this thesis) and its score, measured in the percentage of the overlap match (note: overlaps do not need to be perfect). Each line essentially represents an edge in our graph between two nodes. The first $n$ lines in the file, where $n$ is the number of reads, represent the overlap between the original and virtual file. They are then followed by all other overlaps.

\subsubsection{GFA}
\label{subsubsec:gfa}

Another important file Raven outputs is a Graphical Fragment Assembly (GFA)\footnote{https://gfa-spec.github.io/GFA-spec/GFA1.html} file. It contains similar information to the CSV file, but with some notable additions, namely, instead of just specifying read overlaps, it also contains the whole sequences it uses in the assembly process. It also includes the original read names specified before assembly that contain mutation information. Each line in the file starts with an identifier, all the different ones being listed in the following table:

\begin{center}
	\begin{tabular}{ |c|c| }
		\hline
		\textbf{Type} & \textbf{Comment} \\
		\hline
		\# & Comment \\
		\hline
		H & Header \\
		\hline
		S & Segment \\
		\hline
		L & Link \\
		\hline
		C & Containment \\
		\hline
		P & Path \\
		\hline
		W & Walk \\
		\hline
	\end{tabular}
\end{center}

For our purposes, the file only contained lines starting with the letters S and L. S denotes a segment (sequence) used in the assembly, along with most of its information that was present before assembly (this is notable because it is missing in the CSV file). The lines starting with an L contain lines about sequence overlaps in a similar manner to the CSV file, with the addition of overlap length and mutation information, the latter being crucial for specifying which parent's haplotypes we are connecting with the overlaps. An example of lines starting with an L is in the following example:

\begin{lstlisting}
L	read=4158,reverse,position=395590-406211,length=10621,NC_001139.9|mutated	+ read=4884,reverse,position=383506-395832,length=12326,NC_001139.9|mutated	+	101M
L	read=4832,forward,position=802633-815655,length=13022,NC_001139.9|mutated	-	read=4165,reverse,position=793088-803478,length=10390,NC_001139.9|mutated	+	733M
\end{lstlisting}

The S lines are important because we can use them to associate the GFA file with the CSV file. This is due to the fact that the read index of a true read in the CSV file is equal to the line index of a sequence in the GFA file divided by two. So for example, the read with the true index 346 in the CSV file is equal to the sequence in line 173 in the GFA file.

\subsubsection{Miscellaneous}

Aside from the mentioned file formats, we created numerous other files of our own that either didn't have a suffix, or simply ended with a \textit{.txt} extension, for easier reading and writing. Those files were of an unspecified format and contained temporary information abut graphs, such as overlap lengths, parent affiliation, and similar.

\section{Thesis Task}

The task we are presented with here is slightly different compared to standard genome sequencing. Instead of just finding a path through the assembly graph in order to assemble a complete genome, we are instead tasked with removing edges in the graph that connect contigs belonging to different parents. By doing this, we are essentially separating the two haplotypes that constitute a genome. To give more insight into this, we will explain what graphs are and how we use them in the following section.

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance={15mm}, thick, main/.style = {draw, circle}] 
	\node[main] (1) {$v_1$}; 
	\node[main] (2) [above right of=1] {$v_2$}; 
	\node[main] (3) [below right of=1] {5}; 
	\node[main] (4) [above right of=3] {8}; 
	\node[main] (5) [above right of=4] {3}; 
	\node[main] (6) [below right of=4] {4}; 
	\draw[->] (1) -- node[auto] {$e$} (2);
	\draw[->] (1) -- node[auto] {7} (3);
	\draw[->] (1) to [out=135,in=90,looseness=1.5] node[auto] {1} (5); 
	\draw[->] (1) to [out=180,in=270,looseness=5] node[auto, swap] {3} (1); 
	\draw[->] (2) -- node[auto] {4} (4); 
	\draw[->] (3) -- node[auto] {2} (4); 
	\draw[->] (5) -- node[auto] {8} (4); 
	\draw[->] (5) to [out=315, in=315, looseness=2.5] node[auto] {2} (3); 
	\draw[->] (6) -- node[auto] {1} (4); 
	\end{tikzpicture}
	\caption[Example graph]{An example of a directed graph with node and edge weights. Aside from every edge being directed, loops are also present, which connect nodes to themselves.}
	\label{fig:example_graph}
\end{figure}

\subsection{Graphs}

Graphs are data structures that can be defined with a set of nodes (vertices) $V$ and a set of edges connecting them $E$. Formally, this can be written as:
\[ G = (V, E) \]
where:
\[ E \subseteq 	\{ (x, y)|(x, y) \in V^2 \mathrm{\ and\ } x \neq y \} \]
The above definition is an example of a directed graph, meaning that the edges only go in one direction, which is the type of graph we work with in this thesis. A representation of such a graph can be seen in Figure \ref{fig:example_graph}. For instance, if edge $e$ is connecting nodes $v_1$ and $v_2$ in the direction $v_1 \rightarrow v_2$, then this means that node $v_1$ is connected to node $v_2$, but not the other way around. This is done so because edge direction can encode both suffix - prefix and prefix - suffix overlaps (section \ref{subsec:the process of sequencing and assembly}), which is an useful information for us. Aside from this, both the nodes and edges have weights assigned to them that can represent various information about the graph \cite{trudeau_2017}.

In this thesis, after sequencing and assembly (section \ref{subsec:the process of sequencing and assembly}), we obtain a list of reads connected to each other via overlaps. The reads can be thought of as nodes in a graph, while edges are the links between the nodes. By building such structures, we can more easily use existing graph theory insights to remove unwanted edges and separate the two haplotypes.

\section{Deep Learning}

In this section, we will go over the basics of deep learning (section \ref{subsec:basics}), the main method behind the experiments in this thesis, followed with an overview of graph neural networks (section \ref{subsec:graph neural networks}) and all the different variants of them we used. Lastly, in section \ref{subsec:other functions}, we will explain all the different nonlinearities and regularization functions we used in our models.

\subsection{Basics}
\label{subsec:basics}

In the last decade, deep learning has grown from a niche research area to one of the most prominent fields withing computer science. It is now actively employed in virtually every human endeavor, from healthcare to entertainment \cite{dl_applications}. And it is still growing day by day on its mission to revolutionize the way we handle almost all data \cite{dl_growth}. To give insight into the field, will will explain the most basic building block of DL, Artificial Neural Networks, along with the algorithms that make them work: Stochastic Gradient Descent and Backpropagation.

\subsubsection{Artificial Neural Networks}

In deep learning, we use data processing structures called \textit{artificial neural networks} (ANNs) to extract information from data and learn to predict an outcome, such as some feature of the data or a target class to which the data belongs. An example of a fully connected neural network can be seen in Figure \ref{fig:neural network}. ANNs learn from data by adjusting trainable \textit{weights} that are defined for every connection between neurons in our network. Due to these weights, neural networks are much denser structures when compared to previous machine learning methods and can be referred to as \textit{universal function approximators} \cite{uni_approx} due to their ability to, with large enough networks, approximate any continuous function. This gives them unprecedented performance on previously unsolvable tasks, but due to their abstract structure, makes them somewhat difficult to interpret. Some networks, like \textit{convolutional neural networks} \cite{CNN}, don't suffer from this problem of interpretability as much and can produce some quite intuitive visualizations. On the other hand, some networks, like the ones we will use in this thesis, cannot be visually meaningfully interpreted.

\begin{figure}
	\centering
	\begin{tikzpicture}[x=2.2cm,y=1.4cm]
		\readlist\Nnod{2,3,3,3,2} % array of number of nodes per layer
		\readlist\Nstr{n,m,m,m,k} % array of string number of nodes per layer
		\readlist\Cstr{\strut x,a^{(\prev)},a^{(\prev)},a^{(\prev)},y} % array of coefficient symbol per layer
		\def\yshift{0.5} % shift last node for dots
		
		\foreachitem \N \in \Nnod{ % loop over layers
			\def\lay{\Ncnt} % alias of index of current layer
			\pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
			\message{\lay,}
			\foreach \i [evaluate={\c=int(\i==\N); \y=\N/2-\i-\c*\yshift;
				\index=(\i<\N?int(\i):"\Nstr[\lay]");
				\x=\lay; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes
				% NODES
				\node[node \n] (N\lay-\i) at (\x,\y) {$\Cstr[\lay]_{\index}$};
				
				% CONNECTIONS
				\ifnum\lay>1 % connect to previous layer
				\foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
					\draw[connect,white,line width=1.2] (N\prev-\j) -- (N\lay-\i);
					\draw[connect] (N\prev-\j) -- (N\lay-\i);
					%\draw[connect] (N\prev-\j.0) -- (N\lay-\i.180); % connect to left
				}
				\fi % else: nothing to connect first layer
				
			}
			\path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.5] {$\vdots$};
		}
		
		% LABELS
		\node[above=0.2,align=center,mygreen!60!black] at (N1-1.90) {input\\[-0.2em]layer};
		\node[above=0.2,align=center,myblue!60!black] at (N3-1.90) {hidden layers};
		\node[above=0.2,align=center,myred!60!black] at (N\Nnodlen-1.90) {output\\[-0.2em]layer};
	\end{tikzpicture}
	\caption[Neural network]{Example of a fully connected neural network. Nodes represent individual artificial neurons. The green neurons represent the input to the network, which is of size $n$. The blue neurons represent the hidden layers, which are \textit{fully connected}, i.e. each neuron is connected to every other neuron in the previous and following layer. Each of these connections has a trainable weight associated with it used for learning how the neurons should pass data between them to learn from it. The hidden layers are of size $m$ with the numbers in brackets representing the layer index. Finally, the red neurons represent the output of the network, which is of size $k$. This image was taken from \cite{latex_nn}.}
	\label{fig:neural network}
\end{figure}

\subsubsection{Loss Function \& Stochastic Gradient Descent}
\label{subsubsec: loss function and stochastic gradient descent}

To successfully explain how ANNs work, we need to introduce two concepts: a loss function and backpropagation. When we use our deep learning model to learn from data, we pass the data through our network and compare the output to a previously defined true value by using a \textit{loss function}. A loss function abstracts the error of our network prediction to a single number which is then used to calculate gradients in respect to our data. The loss function we used is described in section \ref{subsec:loss function}. These gradients are then propagated back through the network using \textit{backpropagation}. But, before explaining the backpropagation algorithm, we first need to explain a related, but simpler concept in the form of \textit{Stochastic Gradient Descent} (SGD). In SGD, to find a function's minimum value, we simply nudge all the weights of the function in the direction opposite the function's gradients. This will bring the weights closer to an optimal position as the gradient points in the opposite direction of the function minimum. This is represented in the following equation:

\[ w = w - \frac{\eta}{n} \sum_{i=1}^n \nabla Q_i(w) \]
where $w$ are the function weights, $n$ is the number of data samples, $\eta$ is the learning rate (section \ref{subsec:optimizer}) and $\nabla Q_i(w)$ is the gradient of the function $Q$ over weights $w$.

Now, SGD can also come in multiple forms. True SGD, as in the above equation, calculates these weight changes by first calculating function loss on all data we have available, averages it and only then updates weights. This is very accurate, but also rather slow. On the other end of the spectrum, we can update weights after we process every piece of data. This is very fast and has a lower chance of ending up in local optima, but is also too imprecise for fine-tuning network performance. Due to this, we use a compromise of the two in the form of \textit{mini-batches}. Instead of using only one or all data, we use a batch of data. Using larger batches is slower and more precise, while using smaller batches is faster and less precise. That being said, most often, we are limited by memory constraints while determining batch size.

\subsubsection{Backpropagation}
\label{subsubsec:backpropagation}

We can now move on to the backpropagation algorithm. In essence, it is the idea of SGD applied to neural networks. It tells every weight in our network how to change in order to better predict our data. By correctly propagating these gradient values back through the network, which backpropagation does, we can successfully make our network learn from data \cite{Goodfellow-et-al-2016}. We will now explain this in more detail.

After we obtain a loss value and gradients, we need to somehow pass them back through the network and adjust network weights properly according to those gradients. The amount of change is different depending on the layer of the network, but can be calculated using the partial derivative of the network error in respect to the weight with the chain rule:

\[ \frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial o_j} \frac{\partial o_j}{\partial w_{ij}} = \frac{\partial E}{\partial o_j} \frac{\partial o_j}{\partial net_j} \frac{\partial net_j}{\partial w_{ij}}\]
Here, $E$ is the error of our network, $o_j$ is the output of layer $j$, $net_j$ is the output of layer $j$ before the activation function, and $w_{ij}$ is the weight of the neuron $i$ in layer $j$. Only one part of $net_j$ depends on $w_{ij}$, so we can substitute the last term in the above equation with $o_i$. If the neuron is in the output layer (denoted in a red color in Figure \ref{fig:neural network}), then $o_j = y$, so $\frac{\partial E}{\partial o_{j}} = \frac{\partial E}{\partial y}$. If it isn't (denoted with a blue color in Figure \ref{fig:neural network}), the derivative is less obvious and we won't go into detail on how to obtain it, but will simply state it.

Finally, we get:

\[ \frac{\partial E}{\partial w_{ij}} = \delta_j o_i \]
where:

\[ \delta_j = \frac{\partial E}{\partial o_j} \frac{\partial o_j}{\partial net_j} = \begin{cases}
	\frac{\partial E}{\partial y} \frac{\partial \phi(net_j)}{\partial net_j} & \text{if $j$ is an output neuron,} \\
	(\sum_{l \in L} w_{jl} \delta_l) \frac{\partial \phi(net_j)}{\partial net_j} & \text{if $j$ is an inner neuron.}
\end{cases}
\]
Here, $\phi$ is the activation function and $L$ is the set of all neurons receiving input from node $j$, or in other words, all successor nodes to node $j$. After we have obtained these values, we use the weight update function from the previous chapter to finally update network weights.

\subsection{Graph Neural Networks}
\label{subsec:graph neural networks}

While standard ANNs are great at predicting simple data with no underlying structure (or at least one that isn't known), to successfully make our network learn from assembly graphs, we will need something more refined. Yes, it is true that we can simply represent our graph in the form of a 1-dimensional vector, but we then lose precious structural information about the contig overlaps. By using networks more tailor-made for data representation on graphs, we can take advantage of the graph's underlying structure. The networks in question are called \textit{Graph Neural Networks} (GNNs) \cite{GNN}. Most modern graph neural networks work on the principle of \textit{message passing} \cite{message_passing}. A node accumulates information from adjacent nodes and the edges connecting them and uses it to update its own weights. The information is represented in the form of node and edge features, multidimensional vectors of data that represent some information about the nodes and edges. By repeating this process enough times, we can converge to a stable solution. This can be represented wit the following equation.

%TODO: napisati svojim rijecima

Let $x_v \in \mathbf{R}^{d_1}$ be the feature for node $v$, and $w \in \mathbf{R}^{d_2}$ be the feature for edge ($u$, $v$). The message passing paradigm defines the following node-wise and edge-wise computation at step $t+1$:

Edge-wise: \[ m_e^{(t+1)} = \phi (x_v^{(t)}, x_u^{(t)}, w_e^{(t)}), (u, v, e) \in \mathcal{E} \]

Node-wise: \[ x_v^{(t+1)} = \psi (x_v^{(t)}, \rho (\{m_e^{(t+1)}: (u, v, e) \in \mathcal{E}\})) \]
In the above equations, $\phi$ is a message function defined on each edge to generate a message by combining the edge feature with the features of its incident nodes; $\psi$ is an update function defined on each node to update the node feature by aggregating its incoming messages using the reduce function $\rho$.

\subsubsection{GNNs and CNNs}

The GNN can be though of as an extension of CNNs. A CNN takes an image's local neighborhood and extracts information from it. It does this using convolution \textit{filters}, which take a certain amount of pixels in a neighborhood and multiply them with weights. Now, the size of this filter is predefined and cannot be changed. For instance, it can have a size of 3 x 3 or 5 x 5. If we were to create such a filter for use on graphs, we would simply designate the central weight of the filter to be the node we are currently looking at, and the surrounding weights would be its neighboring nodes. As we can see, this would limit us to graphs where nodes had a constant number of neighbors, or graphs where we could look only at a limited number of neighbors. GNNs do not have this limitation. The $\psi$ function takes all nodes in a central node's neighborhood into account equally \cite{GRL} \cite{bronstein2021geometric}.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/gnn_backprop.png}
	\caption[GNN backpropagation]{A diagram depicting the process of unfolding a graph in order to calculate its gradients. In the first step (top), we have graph. We then build a network in terms of the graph's update functions $g$ and $f$, thereby obtaining an encoding network corresponding to our graph. In the last step (bottom), we unfold the encoding network to get a recurrent neural network where each layer contains all of the graph's functions $f$ and $g$. Image taken from \cite{GNN}.}
	\label{fig:gnn backprop}
\end{figure}

\subsubsection{Backpropagation for GNNs}

Calculating backpropagation for GNNs is different from regular backpropagation (section \ref{subsubsec:backpropagation}). We will here try to explain it intuitively without going into too much mathematical detail, as it's beyond the scope of this thesis and rather unnecessary for understanding how GNNs work. First, it is necessary to look at a GNN as consisting of units of functions $\phi$ and $\psi$. We can then build a network of these functions that corresponds to our graph and call it an \textit{encoding network}. When functions $\phi$ and $\psi$ are implemented by regular neural networks, the encoding networks turn out to be \textit{recurrent neural networks}, i.e. networks that that take as input values from a previous neuron and themselves. After we unfold this network, we get a time series that corresponds to steps in our GNN learning process. Each of these steps contains all of units in an encoding network and defines how they pass messages to each other in each time step. We then apply backpropagation on recurrent neural networks, as defined in \cite{rnn_backprop}. This process can be seen in Figure \ref{fig:gnn backprop} \cite{GNN}.

We used three different GNNs in this thesis, which we will describe in the following sections. We will start with the most basic GNN, the Graph Convolutional Network (section \ref{subsubsec:graph convolutional netwroks}), follow them up with Graph Attention Networks (section \ref{subsubsec:graph attention networks}), and finally finish with the most complex network type, the Edge Graph Attention Network (section \ref{subsubsec:edge graph attention network}).

\subsubsection{Graph Covolutional Networks}
\label{subsubsec:graph convolutional netwroks}

The first network we used was the simplest and oldest one, and it bases its computations on Graph Convolutional Networks (GCN) \cite{GCN}. It aggregates information from neighboring nodes and the edges connecting them and uses them to update the central node's information. It can be defined as follows:

\[ h_i^{(l+1)} = \sigma (b^{(l)} + \sum_{j \in \mathcal{N}(i)} \frac{e_{ji}}{c_{ji}} h_j^{(l)} W^{(l)} ) \]
Here, $\mathcal{N}(i)$ represents all the neighboring nodes of node $i$, $e_{ji}$ is the scalar weight of the edge connecting nodes $i$ and $j$, $c_{ji} = \sqrt{|\mathcal{N}(j)|} \sqrt{|\mathcal{N}(i)|}$, and $\sigma$ is an activation function. $b^{(l)}$, $h_j^{(l)}$ and $W^{(l)}$ are the networks bias at step $l$, features of node $j$ at step $l$ and weight of the network at step $l$ respectively. The aggregated information is used to update the features $h$ of node $i$ at step $(l+1)$. We can see that the calculations use are fairly straightforward, as they can be summed up by simply multiplying the network weight and node features and adding bias before passing it through an nonlinearity. A reader familiar with deep learning may notice that this is similar to an iteration of the backpropagation algorithm, but applied to the graph domain.

\subsubsection{Graph Attention Networks}
\label{subsubsec:graph attention networks}

The second networks we used is more complex than a simple GCN, as it uses the \textit{attention mechanism} \cite{attention} to improve its performance. The Graph Attention Networks (GAT) \cite{GATv2} we used is an updated version of the original GAT \cite{GAT} called GATv2 and we did notice a slight increase in performance while using it. It can be defined as follows:

\[ h_i^{(l+1)} = \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^{(l)} W_{right}^{(l)} h_j^{(l)} \]
where:
\[ \alpha_{ij}^{(l)} = \mathrm{softmax}_i (e_{ij}^{(l)}) \]
\[ e_{ij}^{(l)} = \vec{a}^{T^{(l)}} \mathrm{LeakyReLU} (W_{left}^{(l)} h_i + W_{right}^{(l)} h_j) \]
Here, $W_{right}$ is one of the two network weight matrices, $h$ is a node feature matrix, $\alpha$ is an attention weight and $\vec{a}$ is the attention weight vector. We can see that the basic equation is similar to the GCN, but with an added attention weight. This weight is the main reason why the network performs better compared to the regular GCN. The attention mechanism works by selecting data instances during training, such as nodes, that it considers more important than other data and thereby giving it more weight. This way, it increases model capacity and performance, while at the same time not directly increasing the network size. It is also worth mentioning that we can specify a number of so-called attention heads. Attention heads are simply multiple attention mechanisms at work at the same time. The final attention score is calculated by concatenating or averaging these different attention values. This is done to improve training stability and ultimately performance.

This network differs from the original GAT only in the way $e_{ij}^{(l)}$ is calculated. Instead of using a single weight matrix $W$ that is concatenated to the features $h$, we use two separate matrices in the form of $W_{right}$ and $W_{left}$, giving the network more parameters and learning power.

\subsubsection{Edge Graph Attention Network}
\label{subsubsec:edge graph attention network}

The last network we used, and by far the most effective one, is the graph attention layer that handles edge features (EGAT) \cite{EGAT}. The main equation is the same as in GAT (section \ref{subsubsec:graph attention networks}). The difference lies in how the attention scores $e_{ij}$ are calculated. Instead of the usual way, the are obtained like this:

\[ e_{ij} = \vec{a} f_{ij}^{'} \]
\[ f_{ij}^{'} = \mathrm{LeakyReLU} (A[h_i||f_{ij}||h_j]) \]
Here, $f_{ij}$ represents edge features, $A$ is a weight matrix and $\vec{a}$ is a weight vector. This network greatly improves performance due to the fact that it better utilizes edge features. Instead of just using them as weights to scale edge importance, it handles them in the same manner of importance as the node features. This way, the network can find meaning in the edge weights much better compared to a regular GAT network.

\subsection{Other Functions}
\label{subsec:other functions}

In this section, we will explain all the functions we use in our networks that aren't directly responsible for training, such as regularization functions and nonlinearities.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}
		[
		grid=major,
		axis x line=middle,
		axis y line=middle,
		domain=-5:2,
		legend style={at={(0.37,0.95)}}  
		]
			\addplot+[mark=none,red,domain=-5:0] {0};
			\addplot+[mark=none,red,domain=0:2] {x};
			\addplot+[mark=none,blue,domain=-5:0] {0.1*x};
			\addplot+[mark=none,green,domain=-5:0] {exp(x)-1};
			\addplot+[mark=none,yellow,domain=-5:5] {1/(1+exp(-x))};
			\legend{$\mathrm{ReLU}(x)$,,$\mathrm{LeakyReLU}(x)$,$\mathrm{ELU}(x)$, $\mathrm{sigmoid}(x)$}
		\end{axis}
	\end{tikzpicture}
\caption{An example of all the different nonlinearities used in this thesis for model training. In red, we can see a regular ReLU function, which is valued at zero for $x<0$. The blue line represents the LeakyReLU function, which lets a small value pass through it for $x<0$, here value at $0.1*x$. The green line represents the ELU function, which takes a exponential form for $x<0$. We can observe that these functions are different only for $x<0$. Lastly, the yellow line represents the sigmoid function. Unlike the other functions, it restricts both positive and negative values to  range of $[0,1]$.}
\label{fig:nonlinearities}
\end{figure}

\subsubsection{Nonlinearities}
\label{subsubsec: nonlinearities}

We use four different nonlinearities in our work. Three of them are used as nonlinearities between network layers, and one of them, the sigmoid function, is used for final classification of the edges. First, we are going to describe the network nonlinearities. The most basic one is the \textbf{ReLU} (rectified linear unit) \cite{relu}, represented with the red line in Figure \ref{fig:nonlinearities}. It can be simply described with the following equation:

\[ \mathrm{ReLU}(z) = \max (0, z) \]
In other words, it lets through everything that is positive in a linear fashion, and sets everything else to zero. This is one of the most widely used nonlinearities. It has the same benefits as a sigmoid function or a tanh function (being differentiable), but is less computationally expensive and doesn't suffer from saturated weights for high input values. That being said, it does suffer from the exploding gradients problem because, unlike the sigmoid or tanh, it does not restrict the size of the outputs.

A variant of the ReLU is the \textbf{LeakyReLU} \cite{leakyrelu}, represented with the blue line in Figure \ref{fig:nonlinearities}. It only differs from ReLU in that it allows for a small non zero constant gradient $\alpha$ below zero. It tries to fix the limitation of ReLU where some neurons never express their values due to them always being negative.

Finally, we have the \textbf{ELU} (exponential linear unit) \cite{elu}, represented with the green line in Figure \ref{fig:nonlinearities}. It can be described with the following equation:

\[ \mathrm{ELU}(z) = \begin{cases} 
z & z > 0 \\
\alpha \cdot (e^z - 1) & z \leq 0
\end{cases} \]
For values greater than 0, it emits a constant output. But for any other value, it slowly smooths out bellow zero where it tends to the value $- \alpha$. Just like LeakyReLU, it also tries to negate the previously mentioned weakness of ReLU and can be used as a strong alternative to it.

Lastly, for the classification of the network outputs, we need to normalize them into a $[0, 1]$ range so they can be interpreted as probabilities. For this, we use the \textbf{sigmoid} function, represented with the yellow line in Figure \ref{fig:nonlinearities} and described with the following equation:

\[ S(x) = \frac{1}{1 + e^{-x}} \]
where $x$ is the input to the function. It is worth noting that for cases where there are more than two classes present, we actually use the \textit{softmax} function. It can be described with the following equation:

\[ \sigma(x) = \frac{\exp (x_i)}{\sum_j \exp (x_j)} \]

\begin{figure}
	\centering
	\begin{tikzpicture}[
	node/.style={circle, draw, thick},
	]
	
	\foreach \y in {1,...,5}{
		\node[node] (i\y) at (0,\nodesep*\y) {};
		\node[node, right=\layersep of i\y] (h1\y) {};
		\node[node, right=\layersep of h1\y] (h2\y) {};
	}
	
	\node[node, right=\layersep of h22] (o1) {};
	\node[node, right=\layersep of h24] (o2) {};
	
	\foreach \source in {1,...,5}
	\foreach \dest in {1,...,5}{
		\path[-stealth, thick] (i\source) edge (h1\dest);
		\path[-stealth, thick] (h1\source) edge (h2\dest);
	}
	\foreach \source in {1,...,5}
	\foreach \dest in {1,2}
	\draw[-stealth, thick] (h2\source) -- (o\dest);
	
	\draw[-stealth, thick] (6, 3*\nodesep) -- node[above,font=\Large\bfseries] {dropout} (8.5, 3*\nodesep);
	
	% Boundary
	
	\foreach \y in {1,...,5}
	\node[node, right=12em of h2\y] (di\y) {};
	
	\node[red,font=\huge] at (di1) {$\times$};
	\node[red,font=\huge] at (di3) {$\times$};
	
	\foreach \y in {1,...,5}
	\node[node, right=\layersep of di\y] (dh1\y) {};
	
	\node[red,font=\huge] at (dh11) {$\times$};
	\node[red,font=\huge] at (dh13) {$\times$};
	\node[red,font=\huge] at (dh14) {$\times$};
	
	\foreach \y in {1,...,5}
	\node[node, right=\layersep of dh1\y] (dh2\y) {};
	
	\node[red,font=\huge] at (dh22) {$\times$};
	\node[red,font=\huge] at (dh24) {$\times$};
	
	\node[node, right=\layersep of dh22] (do1) {};
	\node[node, right=\layersep of dh24] (do2) {};
	
	\foreach \source in {2,4,5}
	\foreach \dest in {2,5}
	\draw[-stealth, thick] (di\source) -- (dh1\dest);
	
	\foreach \source in {2,5}
	\foreach \dest in {1,3,5}
	\draw[-stealth, thick] (dh1\source) -- (dh2\dest);
	
	\foreach \source in {1,3,5}
	\foreach \dest in {1,2}
	\draw[-stealth, thick] (dh2\source) -- (do\dest);
	
	\end{tikzpicture}
\caption{An example of dropout. On the network on the right, we can see that some nodes, as well as their connections, have been dropped. This leads to  sparser network and more node specialization. Taken from \cite{latex_nn}.}
\label{fig:dropout}
\end{figure}

\subsubsection{Regularization Functions}
\label{subsubsec:regularization functions}

We primarily used three different regularization functions throughout our networks. In general, these functions help stabilize and speed up network training, as well as often improve performance.

Before imputing our data into the model, we \textbf{normalize} it, i.e. bring its values into a $[0,1]$ range. This is done so that input data from different sources is all in the same range. If this isn't done automatically in the network, we do it with built-in functions. There are many different ways to normalize edge and node data. Input and output node degrees are normalized by subtracting the minimal respective node degree value from them and dividing them with the maximum value which was also reduced with the minimal value, a process known as min-max scaling. Formally, this can be written as:

\[ x' = \frac{x - \min(x)}{\max(x) - \min(x)} \]
Normalizing edge weights is a bit more complicated. The method we use for this is represented with the following equation:

\[ c_{ji} = \left( \sqrt{\sum_{k \in N(j)} e_{jk} } \sqrt{\sum_{k \in N(i)} e_{ki} } \right) \]
$e_{jk}$ and $e_{ki}$ are the weights of the edges from node $j$ to node $k$ and from node $k$ to node $i$ respectively. We divide all of the weights with the calculated weight $c_{ji}$. Intuitively, we multiply the square roots of the sums of weights of the edges connected to the two nodes that are being connected by edge $e_{ji}$. It is worth noting that we usually did not use node degree normalization, as we would transform the node data into a higher abstract dimension anyway, where the individual node degrees would get lost.

More interesting that normalization is \textbf{dropout} \cite{dropout} \cite{dropout2}. Dropout is one of the most popular regularization techniques as it often greatly improves network performance, but at the cost of training speed. It works by randomly omitting some neurons from the neural network along with their connections with some probability $p$. By doing this, neurons get more specialized in detecting different features in the data and don't co-adapt as much. In practice, these neurons aren't actually removed from the network, but instead their weights are simply reduced by multiplying them with $p$, thereby lessening their importance during training and subsequent inference, which has the same effect as removing them but is less computationally expensive. We used a value for $p$ of 0.2 for regular GCN layers (section \ref{subsubsec:graph convolutional netwroks}), and a value of $0.5$ for GAT layers (section \ref{subsubsec:graph attention networks}). A visualization of dropout can be seen in Figure \ref{fig:dropout}.

Lastly, we employed \textbf{batch normalization} \cite{bn}. As previously described, we normalize input values so that the network can train more easily. However, during training, in deeper layers, some of that normalization is lost and neurons have to chase a moving target in order to learn properly from data. This slows down training speed ad reduces performance. To alleviate this, batch normalization is used to normalize the outputs of neurons during training so that the next layers can always expect the same range of values as input, thereby stabilizing and greatly improving training times. It also allows for a much wider range of learning rate values to be used without making the model diverge. As a side effect, by making the layer activations less dependent on the current batch, it adds some noise to training, and thereby acts as a regularizer to the network.

\chapter{Dataset}
\label{ch:dataset}

In this chapter, we will go over the dataset we used for our models. First, we will give an introduction to the dataset we chose to use and why we did so (section \ref{sec:introduction}), and then explain how we generated our own dataset for use in our models (section \ref{sec:dataset creation}).

\section{Introduction}
\label{sec:introduction}

When it comes to deep learning, data can often take a central role in determining the final outcome of the project. Although carefully crafting a predictive model is important, data quality can have a large impact on the model's performance. In this thesis, in order to train our model, we artificially created a dataset based on the brewer's yeast (\textit{Saccharomyces cerevisiae}) genome. For most experimentation purposes, this dataset was only created using the first chromosome of the genome, as using the entire genome required the creation of a much larger dataset that would have significantly slowed down the training and testing process of different models. This larger dataset would have been necessary due to the fact that using the same number of reads for a larger number of chromosomes than just one would essentially dilute our available information for each chromosome, making our models difficult to train. This can be mitigated by the fact that different chromosomes are somewhat similar, as we will see in further experiments. Nevertheless, using only one chromosome proved effective enough for testing out different models, as we will see later in the results chapter (chapter \ref{ch:results}).

\section{Dataset Creation}
\label{sec:dataset creation}

There ar a lot of readily available datasets for experimenting on graphs and genomic data, but for our intents and purposes, we required a custom dataset created from our own data. We will explain in detail how we created this dataset and how we used it in our experiments.

\subsection{Simulating \& Mutating the Reads}

To generate the dataset, we used a multi-step process. The yeast genome was stored in a simple FASTA file (section \ref{subsubsec:fasta and fastq}) with the chromosomes written down in order. In the first step, we would simply extract the first chromosome from the FASTA file. In the next step, in order to simulate a fragmented chromosome like if it was obtained in the process of sequencing, we needed to generate artificial reads from the chromosome. This was done using the \texttt{seqrequester}\footnote{https://github.com/marbl/seqrequester} package and its \texttt{generate} option for generating reads. By setting the \texttt{-nreads} flag to the desired number, we could specify the number of reads the program would generate, and using the \texttt{-distribution} flag, we could choose the desired distribution for the reads (in our case \texttt{pacbio-hifi}). We did this $n$ times, where $n$ is the number of graphs we wanted to generate for training, usually 100. After this was done, we ended up with $n$ files filled with simulated reads. We used a small C++ program\footnote{Courtesy of R. Vaser} to mutate these reads into new ones of the same length with a mutation frequency of 0.01, meaning that every 100 base pairs, a base pair would get mutated into its complement base pair. Now, if the original simulated reads represented the mother's reads, these mutated ones could be thought of as the father's reads. Lastly, we combined the father's and mother's reads from each of the $2n$ ($n$ for the father's and $n$ for the mother's reads) files into a single file by concatenating them, which left us with $n$ files that were ready for the final sequencing process where we would obtain contigs and their overlaps which we could use for training our model.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/graph_example.png}
	\caption[Graph]{Example of a graph obtained after sequencing and assembly. Orange nodes represent mutated reads, while green nodes represent the original reads. We would generate in surplus of 100 of these graphs in order to train our model. This graph was obtained using the Graphia\footnotemark{} graphing tool.}
	\label{fig:graph}
\end{figure}
\footnotetext{https://graphia.app/}

\subsection{Assembling the Graph}
\label{subsec:assembling the graph}

We found that generating 5000 - 10 000 reads gave satisfactory results in combination with the Raven assembler \cite{Vaser}. This is due to the fact that Raven's \texttt{filter} option reduces the number of edges in the graph if a smaller value for the option is specified. While setting the option to 0.99, and using the previously mentioned number of reads, Raven will generate an appropriate number of nodes and edges for training. Setting it to anything lower would create datasets too small for training. Unfortunately, this very low filter value also meant that the overlaps between our contigs were also a minimum of 99\%, making them less of a candidate for usage as edge features. We won't go into detail about why this is so, as it is unclear to us as well.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/separated_haplotypes_before_training.png}
	\caption[Separated graph]{Example of a graph obtained after sequencing and assembly. In this case, the haplotypes are separated. Orange nodes represent mutated reads, while green nodes represent the original reads. We can see the high completeness of the green graph and high fragmentation of the orange graph. This graph was obtained using the Graphia\footnotemark{} graphing tool.}
	\label{fig:separated graph}
\end{figure}
\footnotetext{https://graphia.app/}

After Raven is done with the assembly process, it outputs two files: a GFA file (section \ref{subsubsec:gfa}) and CSV file (section \ref{subsubsec:csv}). They both contain information about the contigs and their overlaps suitable in a format for generating a graph. They also contained different information in each, so to fully utilize all the graph information we have available, we needed to use both files. From the GFA file, we extract information about which contig belongs to the mutated reads and which one to the original simulated ones, as well as contig overlap length information. We extract them to a separate file for easier analyzing. An example of a graph obtained after assembly can be seen in Figure \ref{fig:graph}. Now, as we previously mentioned, our task is to separate the contigs into the father's and the mother's genome. In other words, if we are presented with a graph where nodes represent contigs and edges their overlaps, we need to remove edges between contigs that don't belong to the same parent. So for each edge, we specify if it's "correct" (i.e. it connects two contigs belonging to the same parent) or "incorrect" (it connect two contigs belonging to different parents). We now have a dataset where each edge has a label, as well as a feature in the form of overlap length. The ratio of the non mutated nodes compared to the mutated nodes is on average around 2.5, while the ratio of labels is roughly 40:60, which is fairly balanced and good enough for our task. That being said, after separation, only one of the haplotypes is well defined, with the other one being highly fragmented and incomplete. This can be seen in Figure \ref{fig:separated graph}. We can now use this as input to our model to generate a dataset fit for training.

\chapter{Implementation}

In this chapter, we will go over all the different software components we used for our experiments. First, we will go over the software we used for our deep learning models, as well as all the tools we used to help us (section \ref{sec:technology stack}). After that, we will explain the structure of our model and how all the different components come together (section \ref{sec:code structure}).

\section{Technology Stack}
\label{sec:technology stack}

The main programming language used for this thesis was Python\footnote{https://www.python.org/}, specifically, Python version 3.9. Aside from this, we also used Bash\footnote{https://www.gnu.org/software/bash/} in order to build some scripts to speed up repetitive tasks. Most of the core functionalities of this project were implemented using two Python libraries: PyTorch and DGL. PyTorch\footnote{https://pytorch.org/} probably needs no introduction. It is currently one of the most popular deep learning library used by millions due to its simplicity and versatility \cite{popular_ml}. It is free and open source and maintained by Facebook's AI Research lab. In this project, it was mostly used for its \textit{tensor} data structure, its implementation of nonlinearities such as the ReLU and ELU (section \ref{subsubsec: nonlinearities}), and loss functions such as cross entropy loss and others. It is also used as the underlying architecture for the deep learning layers we used, which will be described in the next paragraph.

The Deep Graph Library\footnote{https://www.dgl.ai/} (DGL) \cite{DGL} is an free and open source deep learning library primarily aimed at the graph neural networks domain. It is maintained by a diverse team of contributors, most of the stemming from the Amazon Web Services team. In this project, it proved itself as a crucial addition to the list of used tools due to its numerous graph oriented functions. We used it mainly for the following features. Firstly, it allowed us to effortlessly and efficiently create graphs from our yeast dataset that were then used for training our models. Secondly, its numerous implemented GNN layers (section \ref{subsec:graph neural networks}) were easy to set up and test out, allowing us to more quickly find the best model for our data. Lastly, due to it being built on the previously mentioned PyTorch library, it had seamless integration with it and could use many of PyTorch's built in functions to help us in training.

Aside from this, we also used a few other libraries to help us in some tasks. We will here shortly list them and describe them.

\begin{itemize}
	\item NumPy\footnote{https://numpy.org/} - a popular Python library focused on efficient and easy mathematical calculations. We mostly used it for its implementation of large arrays

	\item Pandas\footnote{https://pandas.pydata.org/} - a Python data science library with numerous data oriented features. We used it for its CSV saving and loading capabilities

	\item Scikit-learn\footnote{https://scikit-learn.org/stable/} - the most popular machine learning library for Python. We used its easy to use performance metrics, such as F1 score and accuracy

	\item TensorBoard\footnote{https://www.tensorflow.org/tensorboard} - a Python library used for easy visualization of the training process and its metrics
\end{itemize}

\section{Code Structure}
\label{sec:code structure}

The project contained three main files as well as some helper scripts. In the following sections, we will describe these.

\subsection{DGLDataset}

The first thing to do was generate a dataset from the data obtained after sequencing. For this, we used DGLs \texttt{dgl.data.DGLDataset} class, which we inherit. It has multiple purposes, after generating a dataset we can save it and load it by calling the \texttt{save} and \texttt{load} functions. We can also check if there already is a previously saved dataset using the \texttt{has\_cache} function. We can also get an instance of the dataset by calling the \texttt{\_\_getitem\_\_} function, as well as its length using the \texttt{\_\_len\_\_} function. But by far its most important purpose is generating the dataset we will use for training. We do this by loading the previously generated node and edge features and using them to generate a graph. We also encode label information, edge features, and node features. Node features are generated by taking the in and out degrees of every node.

\subsection{Models}

The models we used for training are defined by inheriting the \texttt{torch.nn.Module} class. In each model, we define the layers of the network, as well as the \texttt{forward} function for the gradients calculation. An example of the EGAT (section \ref{subsubsec:edge graph attention network}) model definition code is in the following:

\begin{lstlisting}
class EGATModel(nn.Module):
def __init__(self, node_features, edge_features, lin_dim, hidden_dim, out_dim, n_classes, num_heads):
super(EGATModel, self).__init__()
self.lin_n = nn.Linear(node_features, lin_dim)
self.egat1 = EGATConv(lin_dim, edge_features, hidden_dim, hidden_dim, num_heads=num_heads)
self.egat2 = EGATConv(hidden_dim * num_heads, hidden_dim * num_heads, out_dim, out_dim, num_heads=1)
self.egat3 = EGATConv(out_dim, out_dim, int(out_dim / 2), int(out_dim / 2), num_heads=1)
self.egat4 = EGATConv(int(out_dim / 2), int(out_dim / 2), int(out_dim / 4), int(out_dim / 4), num_heads=1)
self.classify = MLPPredictorEGAT(int(out_dim / 4), n_classes)
self.dp = nn.Dropout(p=0.5)
\end{lstlisting}

We define the network in the following way. First, we initialize a linear layer that transforms the node features (number of in and out node degrees) into a higher dimension of size \texttt{node\_features}. Then, we define four EGAT layers. The first layer is of the largest width \texttt{hidden\_dim} (both for node and edge features), and subsequent layers divide this size by two in each step. Only the first layer used a \texttt{num\_heads} number of attention heads, the rest used only one. This is done to improve training times, as this network proved much slower than the other ones we used. After all the EGAT layers are done with their computations, we pass everything through a classification layer. This layer works by taking the new calculated node and edge features and concatenating them before passing them into a fully connected classification layer. The network then finally outputs a list of class probabilities for every edge in the graph.

You may have noticed that in some layers, we multiply the input dimension for the EGAT layer with \texttt{num\_heads}. This is done so that the number of attention heads is accounted for correctly while passing the results to the next network. The different layer outputs are \textit{flattened}, i.e. concatenated into a smaller dimensional vector before being passed to the next layer. Aside from this, we also employ the \textit{dropout} (section \ref{subsubsec:regularization functions}) mechanism to reduce overfitting (section \ref{subsec:the training process}).

\subsection{Main}

All of the previously described functions are connected in the \texttt{main} function. First, we define the function for logging parameters such as loss and accuracy for visualization using TensorBoard (section \ref{sec:technology stack}). Then, we define the dataset by calling the \texttt{DGLDataset} module and, if it previously hasn't been generated, create the dataset. We then define the model we will use for training with all its parameters and layer sizes. After that, we define the optimizer for our model, and finally, we start the training process. This process is depicted in Figure \ref{fig:architecture}.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/model.png}
	\caption[Model]{Graphical representation of the model architecture with EGAT layers. Different layers are represented with different colors and the main model is described within the gray box. This image was constructed using diagrams.net\footnotemark{}}
	\label{fig:model}
\end{figure}
\footnotetext{https://www.diagrams.net/}

\section{Model Description}
\label{sec:model description}

In Figure \ref{fig:model}, we can see the general structure of one of our models, specifically, the model using the EGAT layers (section \ref{subsubsec:edge graph attention network}). We will explain this model in detail as it is the most complicated one we used and it generalizes well to all other models we used.

In the first step, we take node features, node in and out degrees, as input to a linear layer. The dimension of each of these feature vectors is $n$, where $n$ is the number of nodes in the current graph. The purpose of this linear layer is to increase the size of the features from two to a larger number $m_1$ (usually 64 or 128), thereby extracting useful information from them and successfully representing it with a suitable dimension.

We then have a $n \times m_1$ matrix, which we use as input to the first EGAT layer along with the node features we use (edge overlap lengths) and the graph structure. The EGAT layer uses these to perform its operations and outputs two vectors of size $n \times m_2 \times a$ and $e \times m_2 \times a$, where $m_2$ is the output size of the first EGAT layer (up to four times alrger than $m_1$), $a$ is the number of attention heads (usually three) and $e$ is the number of edges. The two vectors represent the learned features of the nodes and edges respectively. They are then passed through ELU activation function (section \ref{subsubsec: nonlinearities}) which acts as a nonlinearity, followed with the dropout mechanism (section \ref{subsubsec:regularization functions}). This is then finally passed through batch normalization (section \ref{subsubsec:regularization functions}).

This set of operations represents one step of the network, which we perform four times. In each step, we reduce the $m_2$ dimension two times in each step and use only one attention head. After the final layer is done, we pass everything through a classification layer. This layer works by taking the last set of calculated node and edge features and concatenates them in a node feature, edge feature, node feature order. These are then used as input to a fully connected layer which finally outputs logits. To use them in a loss function, we pass them through a sigmoid function to get class probabilities.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/system_architecture.png}
	\caption[Architecture]{Illustration depicting the architecture of our system. All the different components are joined together in the Main program. This image was constructed using diagrams.net\footnotemark{}}
	\label{fig:architecture}
\end{figure}
\footnotetext{https://www.diagrams.net/}

\subsection{Optimizer}
\label{subsec:optimizer}

An important element of every machine learning system is the optimizer. An optimizer is a function that controls how the weights of our network are adjusted after we obtain the gradients. The optimizer we used is the popular Adam (Adaptive Moment Estimation) optimizer \cite{adam}. It falls into the adaptive optimizers category and represents a nice balance between speed and performance due to using two different adaptive techniques: momentum and root mean square propagation (RMSProp). Recently, there has been more and more research about the advantages and disadvantages of adaptive optimization techniques compared to standard \textit{Stochastic Gradient Descent} (SGD). However, it is still a good idea to first use Adam as it doesn't require hyperparameter tuning to offer good results \cite{optimizers}. Momentum accelerates gradient descent using the \textit{exponentially weighted average} of the gradients and can be described with the following equations:

\[ w_{t+1} = w_t - \alpha m_t \]
where

\[ m_t = \beta m_{t-1} + (1 - \beta)  \frac{\partial L}{\partial w_t} \]
Here, $w_t$ and $w_{t+1}$ are the current and new weights respectively and $\alpha$ is the learning rate. $m_t$ and $m_{t-1}$ are the current and previous moving averages respectively, while $\frac{\partial L}{\partial w_t}$ is the derivative of the loss function by the weights of the network. Lastly, $\beta$ is the moving average parameter or decay value, usually 0.9. Essentially, momentum accumulates previous gradients to speed up its convergence towards an optimum. On the other hand, RMSProp can be described as:

\[ w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t + \epsilon}} \frac{\partial L}{\partial w_t} \]
where

\[ v_t = \beta v_{t-1} + (1 - \beta) \left(\frac{\partial L}{\partial w_t}\right)^2 \]
Here, $v_t$ and $v_{t-1}$ are the current and previous sum of squared gradients (moving averages) and $\epsilon$ is a small value added to avoid division by zero. RMSProp works the same way as momentum, but instead uses the squares of gradients and instead of multiplying $\alpha$ with $v_t$, it divides by it.

Finally, Adam is defined with the following set of equations:

\[ m_i = \beta_1 m_i + (1 - \beta_1) \frac{\partial L}{\partial \theta_i} \]


\[ v_i = \beta_2 v_i + (1 - \beta_2) \left(\frac{\partial L}{\partial \theta_i}\right)^2 \]
then:

\[ \widehat{m_l} = \frac{m_i}{1 - \beta_1}, \widehat{v_l} = \frac{v_i}{1 - \beta_2} \]

\[ \theta_i = \theta_i - \frac{\alpha}{\sqrt{\widehat{v_l}} + \epsilon} \widehat{m_l} \]
It uses both the first and the second momentum which are divided by one minus the decay factor $\beta$ to account for bias in the estimator. $\epsilon$ is a small value added in order to avoid division by zero and $\alpha$ is the learning rate. $\beta_1$ is almost always 0.9, while $\beta_2$ is usually 0.999. In the first iteration, the moving averages are set to zero. By using techniques from both SGD with momentum and RMSProp, Adam inherits both of their strengths. The main advantages are that Adam takes big enough steps to avoid local optima, while simultaneously oscillating minimally when it reaches the global optimum.

During training, we used a version od Adam implemented in PyTorch (section \ref{sec:technology stack}) with default parameters. The learning rate was set to 0.001, the $\beta$ parameters were set to 0.9 an 0.999 respectively, $\epsilon$ was set to 1e-8 and weight decay wasn't used.

\subsection{Training and Validation}
\label{subsec:the training process}

After we generated our dataset of $n$ graphs (100 for the dataset with only one chromosome), we used 99 graphs for training and one for validation. This is done to prevent \textit{overfitting}. This can be explained in the following way. As our model trains on the training part of the dataset, it starts to memorize the data in our dataset by heart. This may sound good, but it actually leads to poor \textit{generalization}, i.e., it cannot perform well on other similar data, only on the data it was trained on. To alleviate this, we separate a smaller validation dataset from the main one that the model doesn't train on. By validating our performance on it, we can get a more accurate state of the performance of our model. As the model starts overfitting, performance on the train part of the dataset will continue to improve, while on the validation part, it will start to worsen. We can then take the step where the model performed on the validation dataset the best as the best version of our model.

Aside from this, validation can also be used for tuning \textit{hyperparameters} of our model, i.e. parameters that aren't adjusted during training, but are set before training. In that case, as some of the information from the validation dataset leaks into the training,  we also need to create a test dataset that will now fulfill the role that validation has fulfilled previously: generalization testing. In this thesis, we don't do this as most of the hyperparameters we used are set to default and we aren't concerned with top performance as much as testing out if the concept of haplotype separation works with GNNs and with what models.

The models were trained on a setup with two AMD EPYC 7662 64-Core processors with 32 threads. Training time was shortest for GCNs, with around 30 minutes for 100 epochs. It took up to two times as much time for 100 epochs of GATs and four times as much for EGATs.

\subsection{Hyperparameters}
\label{subsec:hyperparameters}

There are numerous important hyperparameters that can be adjusted to optimize training performance. However, due to time constraints and simplicity reasons, we mostly used default parameters in this thesis. We already mentioned the hyperparameters we used for the Adam optimizer (section \ref{subsec:optimizer}), the network sizes we used (section \ref{sec:model description}), as well as the values used for dropout (section \ref{subsubsec:regularization functions}), but there are others that require an explanation. An important parameter is batch size (section \ref{subsubsec: loss function and stochastic gradient descent}). Batch size determines how much of our data enters our network during one step of training. In GNNs, it translates to the number of separate graphs in an instance of the dataset. The message passing algorithm (section \ref{subsec:graph neural networks}) doesn't pass data over from separate graphs, so batches can be implemented very easily. That being said, the graphs we used for training were of sufficient size and using more than one graph during an instance of training simply wasn't necesary. In addition, some graphs appeared fragmented after assembly anyways, so they acted like mini-batches regardless.

\chapter{Results}
\label{ch:results}

In this chapter, we will present the results obtained in the experiments we performed for this thesis. First, we will go over the experiments er performed on only one chromosome (section \ref{sec:experiments on one chromosome}), and then over the experiments we performed on the whole genome (section \ref{sec:experiments on the whole genome}).

\section{Performance Metrics}

Here, we will briefly explain all the different metrics for evaluating model performance during training that we used. These include accuracy (section \ref{subsec:accuracy}), F1 score along with recall and precision (section \ref{subsec:f1 score}) and model loss (section \ref{subsec:loss function}).

\subsection{Accuracy}
\label{subsec:accuracy}

Accuracy is a simple and reliable metric used for assessing model performance that is quite popular and widespread due to its interpretability and easy to understand nature. It can be described as the number of correctly classified examples divided by the total number of examples in the dataset. With that being said, it does have its limits. In the case when the dataset is severely imbalanced, i.e. there are siginificantly more examples of one class compared to the other, e.g. \textit{true} vs. \textit{false}, it will always display a high value by simply always guessing \textit{true}. Fortunately, this is not the case for our dataset, as we have a fairly similar number of correct and incorrect examples (section \ref{ch:dataset}). Accuracy can also be defined with the following equation:

\[ \mathrm{Accuracy} = \frac{\mathrm{TP} + \mathrm{TN}}{\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}} \]
Put into words, it is the number of \textit{true positive} examples (everything our model classified correctly) and \textit{true negative} examples (everything our model did not classify correctly) divided by all examples in the dataset.

\subsection{F1-Score}
\label{subsec:f1 score}

F1-score is a more complex metric than the previously described accuracy score, but is also more informative about model results. It belongs to a family of scores known as \textit{F-scores}, which can be described with the following equation:

\[ F_1 = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\beta^2 \cdot \mathrm{precision} + \mathrm{recall}} \] where \[ \mathrm{precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}} \] \[ \mathrm{recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}} \]
Precision measures how many of the examples we classified as positive are truly positive, while recall measures how many of the positive examples present in the dataset our model actually classified as positive.

F1-score is a version of F-score where the $\beta$ parameter is set to 1, which in turn simplifies the equation to:

\[ \mathrm{F_1} = 2 \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}} \]
Intuitively, it can be described as the harmonic mean between the precision and recall metrics and is often used as a more data agnostic version of those metrics.

\subsection{Loss Function}
\label{subsec:loss function}

As our model falls into the classification category, for the loss function, we used \textit{cross entropy loss}, also know as \textit{logistic loss}. It works by taking the \textit{logits} our model generates as output and compares them against true labels. Logits are vectors of size $n$, where $n$ is the number of classes we have predicted for every data entry. For instance, if we have three examples in our dataset that we need to classify into seven classes, the size of our matrix would be $7 \times 3$. This matrix is compared to the \textit{one-hot} encoded classes of the data. One-hot encoded vectors are also of size $n$, but with only their true class for the example set to 1, and everything else to 0. They are compared with the following equation:

\[ L = - \sum_{i = 1}^n t_i \log (p_i) \]
where $t_i$ is either a 1 or a 0 (truth label) for class $i$, and $p_i$ is the probability for class $i$. If we are only using two classes (which we indeed are), we can simplify this to \textit{binary cross entropy loss} represented with the following equation:

\[ L = - [t \log (p) + (1 - t) \log (1 - p)] \]
A smaller value is better, so by minimizing this loss, we can train our model. Due to its logarithmic nature, it heavily penalizes values straying from 0, which is the ideal value of the function \cite{CEL}.

It is also worth noting that the loss function doesn't take logits as input directly, but class probabilities instead, obtained by passing the logits through a softmax function (section \ref{subsubsec: nonlinearities}).

\section{Experiments on One Chromosome}
\label{sec:experiments on one chromosome}

The results of the experiments we performed on one chromosome are presented through three graphs: accuracy, F1 score and loss. We will only display validation graphs, as training graphs are often an unreliable indicator of performance due to them almost certainly overfitting. In addition, we will only display a few different experiments we performed for clarity, even though there were many more. Lastly, we only display the first 200 epochs of training, as all of the models reached their peak performance up to that point.

\subsection{Accuracy \& F1 Score}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/accuracy_val.png}
	\includegraphics[width=\textwidth]{images/f1_val.png}
	\caption[Accuracy and f1 graph]{Two graphs representing the model training performance on the validation part of the dataset. Above, we have the accuracy graph and below it, we can see an F1 score graph. Epochs are displayed on the $x$ axis, while on the $y$ axis, we have the respective performance metric of either accuracy or F1 score. The graphs were obtained using TensorBoard (section \ref{sec:technology stack}).}
	\label{fig:accuracy and f1 graph}
\end{figure}

The first two graphs we are going to look at are an accuracy graph and an F1 score graph (Figure \ref{fig:accuracy and f1 graph} top, Figure \ref{fig:accuracy and f1 graph} bottom). We are going to analyze both of them at the same time due to their similarity. There are four model results displayed here. In red, a GATv2 model (section \ref{subsubsec:graph attention networks}) with dropout set to 0.5 and edge overlaps used as edge weights in the GCN layers is displayed. In blue, we have the same model, but without using edge overlaps. Their performance is nearly similar, proving that this way of using edge overlaps is rather inefficient. In orange, we use GCN model (section \ref{subsubsec:graph convolutional netwroks}) with dropout set to 0.2 and edge overlaps used. We can see that it is of a very similar performance, meaning that the GAT layers extract little additional information from the model.

The standout model here is the pink model. It has greatly outperformed the rest of the models, with an accuracy of 87.9\%. This is because here we used an EGAT model (section \ref{subsubsec:edge graph attention network}), which uses edge features much more effectively by incorporating them into the attention mechanism. However, we have to note that we had to use a much smaller model here due to slow training times. Nevertheless, it proved effective for the task at hand.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/accuracy_eg.png}
	\includegraphics[width=\textwidth]{images/f1_eg.png}
	\includegraphics[width=\textwidth]{images/loss_eg.png}
	\caption[Accuracy,f1 score and loss graph]{Graphs representing the accuracy (top), F1 score (middle) and model loss (bottom) for the validation part of the entire genome dataset. The model was trained for 500 epochs. The graphs were obtained using TensorBoard (section \ref{sec:technology stack}).}
	\label{fig:accuracy, f1 score and loss}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/separated_haplotypes_after_training.png}
	\caption[Separated graph after training]{Example of a graph obtained after training with separated haplotypes. Orange nodes represent mutated reads, while green nodes represent the original reads. We can see that the green graph in the middle is complete, but contaminated with orange nodes, while the orange nodes forming a circle around it aren't contaminated with green nodes at all, but are instead highly fragmented. This graph was obtained using the Graphia\footnotemark{} graphing tool.}
	\label{fig:separated graph after training}
\end{figure}
\footnotetext{https://graphia.app/}

\section{Experiments on the Whole Genome}
\label{sec:experiments on the whole genome}

In addition to accuracy, F1 score and loss, we also present recall and precision. These additional metrics might provide clarity in how the model trains on the provided data. The model we used for training was the EGAT model (section \ref{subsubsec:edge graph attention network}) with default parameters. We use it here because it proved to be the most effective model from all the tested models.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{images/precision_eg.png}
	\includegraphics[width=\textwidth]{images/recall_eg.png}
	\caption[Precision and recall graph]{Graphs representing the precision (top) and recall (bottom) for the validation part of the entire genome dataset. The graphs were obtained using TensorBoard (section \ref{sec:technology stack}).}
	\label{fig:precision and recall}
\end{figure}

\subsection{Accuracy, F1 Score \& Loss}

In Figure \ref{fig:accuracy, f1 score and loss} we can see the graphs for accuracy (top), F1 score (middle) and model loss (bottom). We group these three graphs together due to them showing similar properties. Accuracy and F1 score exhibit almost the same graphs, while model loss is essentially an inverted version of the same graph. They are also grouped together due to them showing a nice example of overfitting. The model reaches its best performance fairly early during training, around epoch 100, and then continues to progressively exhibit worse and worse performance on the validation dataset. The best model had an F1 score of 0.8024, accuracy of 0.7911 and loss of 0.3868. This is not far off from the best performing model on only one chromosome, showing that different chromosomes exhibit similar features which our model could take leverage of.

\subsection{Precision \& Recall}

In Figure \ref{fig:precision and recall} we can see the graphs for precision (top) and recall (bottom) for the validation part of the entire genome dataset. An interesting observation we can immediately see is that at the beginning of training, the model has a near perfect recall with a fairly lower precision score. This means that the model classifies most positive examples in the dataset as indeed positive, while classifying some negative examples incorrectly. As training continues, the model starts trading recall for a higher precision value, and this carries on for the entirety of the 500 epochs. The model essentially tries to balance these two metrics as best as it can, with the optimal balance found somewhere around epoch 100. For that balance, recall stands at 0.9368, while precision is 0.7389. The dataset is of course not perfectly balanced, and this is somewhat reflected in these numbers, but it is also a testament to the fact that the model is simply better at finding incorrect edges, while simultaneously classifying some correct edges as incorrect. Balancing these parameters depends on the use case for such an application, but we can generalize that the model is quite successful at separating haplotypes, but the haplotypes it separates will be somewhat incomplete. This can be seen in Figure \ref{fig:separated graph after training}. It is quire similar to Figure \ref{fig:separated graph}, but with some major differences. The large haplotype in the middle in green is somewhat contaminated with orange nodes, while all the fragmented orange nodes around it don't contain any green nodes. This again confirms our previous recall vs precision findings.

\chapter{Conclusion}


\bibliography{literatura}
\bibliographystyle{abbrv}

\clearpage

\title{Using Graph Neural Networks to Separate Haplotypes in Assembly Graphs}
\begin{abstract}

\keywords{Bioinformatics, Graph Neural Networks, GNN, Haplotype Separation, Deep Learning, Machine Learning}
\end{abstract}

\hrtitle{Korištenje graf neuronskih mreža za odvajanje haplotipa u grafovima astavljanja}
\begin{sazetak}

\kljucnerijeci{Bioinformatika, Graf Neuronske Mreže, GNN, Odvajanje haplotipa, Duboko Učenje, Strojno Učenje}
\end{sazetak}

\end{document}